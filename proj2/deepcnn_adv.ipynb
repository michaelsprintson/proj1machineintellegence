{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchattacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset_orig = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainset, valset = random_split(trainset_orig, (0.8, 0.2))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_block_maker = lambda in_c, out_c, k_s, s, p: nn.Sequential(\n",
    "    nn.Conv2d(in_c, out_c, k_s, s, p),\n",
    "    nn.BatchNorm2d(out_c),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "inception_block_maker = lambda in_c, filt_1, filt_3, filt_5, filt_3_r, filt_5_r, p: [\n",
    "    conv_block_maker(in_c, filt_1, 1, 1, 0), #block 1\n",
    "\n",
    "    nn.Sequential(conv_block_maker(in_c, filt_3_r, 1, 1, 0), #block 2\n",
    "    conv_block_maker(filt_3_r, filt_3, 3, 1, 1)), #block 2\n",
    "    \n",
    "    nn.Sequential(conv_block_maker(in_c, filt_5_r, 1, 1, 0), #block 3\n",
    "    conv_block_maker(filt_5_r, filt_5, 5, 1, 2)), #block 3\n",
    "\n",
    "    nn.Sequential(nn.MaxPool2d((3,3), (1,1), (1,1)),\n",
    "    conv_block_maker(in_c, p, 1, 1, 0)), #block 4\n",
    "]\n",
    "\n",
    "class inception_block(nn.Module):\n",
    "    def __init__(self, in_c, filt_1, filt_3, filt_5, filt_3_r, filt_5_r, p):\n",
    "        super(inception_block, self).__init__()\n",
    "        self.block_1 = conv_block_maker(in_c, filt_1, 1, 1, 0)\n",
    "        self.block_2 = nn.Sequential(conv_block_maker(in_c, filt_3_r, 1, 1, 0), #block 2\n",
    "    conv_block_maker(filt_3_r, filt_3, 3, 1, 1))\n",
    "        self.block_3 = nn.Sequential(conv_block_maker(in_c, filt_5_r, 1, 1, 0), #block 3\n",
    "    conv_block_maker(filt_5_r, filt_5, 5, 1, 2))\n",
    "        self.block_4 = nn.Sequential(nn.MaxPool2d((3,3), (1,1), (1,1)),\n",
    "    conv_block_maker(in_c, p, 1, 1, 0))\n",
    "    def forward(self, t):\n",
    "        First_Block_Out = self.block_1(t)\n",
    "        Second_Block_Out = self.block_2(t)\n",
    "        Third_Block_Out = self.block_3(t)\n",
    "        Fourth_Block_Out = self.block_4(t)\n",
    "        return torch.cat([First_Block_Out,Second_Block_Out, Third_Block_Out, Fourth_Block_Out], dim=1)\n",
    "\n",
    "aux_class = lambda in_c,num_c: nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d((4,4)),\n",
    "    nn.Conv2d(in_c, 128, 1, 1, 0),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(2048,1024),\n",
    "    nn.Dropout(0.7),\n",
    "    nn.Linear(1024, num_c),\n",
    ")\n",
    "\n",
    "class inception_model(nn.Module):\n",
    "    def __init__(self, out_c):\n",
    "        super(inception_model, self).__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            conv_block_maker(3,64,7,2,3),\n",
    "            nn.MaxPool2d((3,3),stride=2,padding=0,ceil_mode =True),\n",
    "            conv_block_maker(64,64,1,1,0),\n",
    "            conv_block_maker(64,192,3,1,1),\n",
    "            nn.MaxPool2d((3,3),stride=2,padding=0,ceil_mode =True),\n",
    "            inception_block(192,64,128,32,96,16,32),\n",
    "            inception_block(256,128,192,96,128,32,64),\n",
    "            nn.MaxPool2d((3,3),stride=2,padding=0,ceil_mode =True),\n",
    "            inception_block(480,192,208,48,96,16,64),)\n",
    "        self.block_2 = nn.Sequential(\n",
    "            inception_block(512,160,224,64,112,24,64),\n",
    "            inception_block(512,128,256,64,128,24,64),\n",
    "            inception_block(512,112,288,64,144,32,64),)\n",
    "        self.block_3 = nn.Sequential(\n",
    "            inception_block(528,256,320,128,160,32,128),\n",
    "            nn.MaxPool2d((3,3),stride=2,padding=0,ceil_mode =True),\n",
    "            inception_block(832,256,320,128,160,32,128),\n",
    "            inception_block(832,384,384,128,192,48,128),\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1,1)),\n",
    "            nn.Flatten(1),\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(in_features =1024,out_features = out_c))\n",
    "        self.aux_1 = aux_class(512,out_c)\n",
    "        self.aux_2 = aux_class(528,out_c)\n",
    "    def forward(self, t):\n",
    "        t = self.block_1(t)\n",
    "        aux_1_out = self.aux_1(t)\n",
    "        t = self.block_2(t)\n",
    "        aux_2_out = self.aux_2(t)\n",
    "        t = self.block_3(t)\n",
    "        if self.training:\n",
    "            return t, aux_1_out, aux_2_out\n",
    "        else:\n",
    "            return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Validate_Model(Model, Train_Loader, Val_Loader, Num_Of_Train_Samples, Num_Of_Val_Samples, Criterion,Optimizer, Num_Epochs):\n",
    "    # ==========================Training Part ================================#\n",
    "    Train_Loss_History, Train_Accuracy_History, Val_Loss_History, Val_Accuracy_History = [], [], [], []\n",
    "    for epoch in (pbar:=tqdm(range(Num_Epochs))):\n",
    "        Num_Of_Predicted_Correctly, Train_Cummulative_Loss = 0, 0\n",
    "        for batch_samples,targets in Train_Loader:\n",
    "            batch_samples = batch_samples.to(device=Device)\n",
    "            targets = targets.to(device=Device)\n",
    "\n",
    "            Network_Predictions, Aux_1_Predictions, Aux_2_Predictions = Model(batch_samples)\n",
    "            Main_Loss =  Criterion(Network_Predictions, targets) + (0.3 * Criterion(Aux_1_Predictions, targets)) + (0.3 * Criterion(Aux_2_Predictions, targets))\n",
    "\n",
    "            Optimizer.zero_grad() \n",
    "            Main_Loss.backward() \n",
    "            Optimizer.step()\n",
    "            \n",
    "            _, Train_Samples_Predictions = Network_Predictions.max(1)\n",
    "            Train_Samples_Predictions = Train_Samples_Predictions.to(device=Device)\n",
    "            Num_Of_Predicted_Correctly += (Train_Samples_Predictions == targets).float().sum().item()\n",
    "            Train_Cummulative_Loss += Main_Loss.data.item() * batch_samples.shape[0]\n",
    "            \n",
    "        Train_Cummulative_Loss /= Num_Of_Train_Samples\n",
    "        Train_Loss_History.append(Train_Cummulative_Loss)\n",
    "        Train_Accuracy = Num_Of_Predicted_Correctly / Num_Of_Train_Samples\n",
    "        Train_Accuracy_History.append(Train_Accuracy)\n",
    "    # =============================== End of Training Part ========================#\n",
    "    \n",
    "    #============================ Validation Part ===================================#\n",
    "    \n",
    "        Num_Of_Predicted_Correctly = 0\n",
    "        with torch.no_grad(): # we're in test mode so we don't need to calc the gradients\n",
    "            Val_Cummulative_Loss = 0\n",
    "            for batch_samples,targets in Val_Loader:\n",
    "                batch_samples = batch_samples.to(device=Device)\n",
    "                targets = targets.to(device=Device)\n",
    "\n",
    "                Network_Predictions, Aux_1_Predictions, Aux_2_Predictions = Model(batch_samples)\n",
    "\n",
    "                Main_Loss = Criterion(Network_Predictions, targets) + (0.3 *  Criterion(Aux_1_Predictions, targets)) + (0.3 * Criterion(Aux_2_Predictions, targets))\n",
    "\n",
    "                _, Val_Samples_Predictions = Network_Predictions.max(1)\n",
    "                Val_Samples_Predictions = Val_Samples_Predictions.to(device=Device)\n",
    "                Num_Of_Predicted_Correctly += (Val_Samples_Predictions == targets).float().sum().item()\n",
    "                Val_Cummulative_Loss += Main_Loss.data.item() * batch_samples.shape[0]\n",
    "            \n",
    "            Val_Cummulative_Loss /= Num_Of_Val_Samples\n",
    "            Val_Loss_History.append(Val_Cummulative_Loss)\n",
    "            Val_Accuracy = Num_Of_Predicted_Correctly / Num_Of_Val_Samples\n",
    "            Val_Accuracy_History.append(Val_Accuracy)\n",
    "            # =============================== End of Validation Part ========================#\n",
    "\n",
    "            pbar.set_description(f\"train-loss:{Train_Cummulative_Loss},train=acc:{Train_Accuracy},val-loss:{Val_Cummulative_Loss},val-loss:{Val_Accuracy}\")\n",
    "            if epoch % 10 == 0:\n",
    "                print()\n",
    "    torch.save(Model.state_dict(), \"InceptionNet_Model\")\n",
    "    return Train_Accuracy_History, Val_Accuracy_History, Train_Loss_History, Val_Loss_History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Validate_Model_Adversarial_Training(Model, Train_Loader, Val_Loader, Num_Of_Train_Samples, Num_Of_Val_Samples, Criterion,Optimizer, Num_Epochs, epsilon):\n",
    "    # ==========================Training Part ================================#\n",
    "    Train_Loss_History, Train_Accuracy_History, Val_Loss_History, Val_Accuracy_History = [], [], [], []\n",
    "    for epoch in (pbar:=tqdm(range(Num_Epochs))):\n",
    "        Num_Of_Predicted_Correctly, Train_Cummulative_Loss = 0, 0\n",
    "        for batch_samples,targets in Train_Loader:\n",
    "            batch_samples = batch_samples.to(device=Device)\n",
    "            targets = targets.to(device=Device)\n",
    "\n",
    "            batch_samples.requires_grad = True\n",
    "            Network_Predictions, Aux_1_Predictions, Aux_2_Predictions = Model(batch_samples)\n",
    "            Main_Loss =  Criterion(Network_Predictions, targets) + (0.3 * Criterion(Aux_1_Predictions, targets)) + (0.3 * Criterion(Aux_2_Predictions, targets))\n",
    "\n",
    "            #Find the loss of unperturbed data and step\n",
    "            Optimizer.zero_grad() \n",
    "            Main_Loss.backward() \n",
    "            Optimizer.step()\n",
    "\n",
    "            #Alter the data with FGSM attack\n",
    "            batch_grad = batch_samples.grad.data\n",
    "            batch_samples_perturbed = torch.clamp(batch_samples + epsilon * batch_grad.sign(), 0, 1)\n",
    "\n",
    "            #Calculate new loss and step\n",
    "            Network_Predictions, Aux_1_Predictions, Aux_2_Predictions = Model(batch_samples_perturbed)\n",
    "            Main_Loss =  Criterion(Network_Predictions, targets) + (0.3 * Criterion(Aux_1_Predictions, targets)) + (0.3 * Criterion(Aux_2_Predictions, targets))\n",
    "\n",
    "            Optimizer.zero_grad()\n",
    "            Main_Loss.backward()\n",
    "            Optimizer.step()\n",
    "            \n",
    "            _, Train_Samples_Predictions = Network_Predictions.max(1)\n",
    "            Train_Samples_Predictions = Train_Samples_Predictions.to(device=Device)\n",
    "            Num_Of_Predicted_Correctly += (Train_Samples_Predictions == targets).float().sum().item()\n",
    "            Train_Cummulative_Loss += Main_Loss.data.item() * batch_samples.shape[0]\n",
    "            \n",
    "        Train_Cummulative_Loss /= Num_Of_Train_Samples\n",
    "        Train_Loss_History.append(Train_Cummulative_Loss)\n",
    "        Train_Accuracy = Num_Of_Predicted_Correctly / Num_Of_Train_Samples\n",
    "        Train_Accuracy_History.append(Train_Accuracy)\n",
    "    # =============================== End of Training Part ========================#\n",
    "    \n",
    "    #============================ Validation Part ===================================#\n",
    "    \n",
    "        Num_Of_Predicted_Correctly = 0\n",
    "        with torch.no_grad(): # we're in test mode so we don't need to calc the gradients\n",
    "            Val_Cummulative_Loss = 0\n",
    "            for batch_samples,targets in Val_Loader:\n",
    "                batch_samples = batch_samples.to(device=Device)\n",
    "                targets = targets.to(device=Device)\n",
    "\n",
    "                Network_Predictions, Aux_1_Predictions, Aux_2_Predictions = Model(batch_samples)\n",
    "\n",
    "                Main_Loss = Criterion(Network_Predictions, targets) + (0.3 *  Criterion(Aux_1_Predictions, targets)) + (0.3 * Criterion(Aux_2_Predictions, targets))\n",
    "\n",
    "                _, Val_Samples_Predictions = Network_Predictions.max(1)\n",
    "                Val_Samples_Predictions = Val_Samples_Predictions.to(device=Device)\n",
    "                Num_Of_Predicted_Correctly += (Val_Samples_Predictions == targets).float().sum().item()\n",
    "                Val_Cummulative_Loss += Main_Loss.data.item() * batch_samples.shape[0]\n",
    "            \n",
    "            Val_Cummulative_Loss /= Num_Of_Val_Samples\n",
    "            Val_Loss_History.append(Val_Cummulative_Loss)\n",
    "            Val_Accuracy = Num_Of_Predicted_Correctly / Num_Of_Val_Samples\n",
    "            Val_Accuracy_History.append(Val_Accuracy)\n",
    "            # =============================== End of Validation Part ========================#\n",
    "\n",
    "            pbar.set_description(f\"train-loss:{Train_Cummulative_Loss},train=acc:{Train_Accuracy},val-loss:{Val_Cummulative_Loss},val-loss:{Val_Accuracy}\")\n",
    "            if epoch % 10 == 0:\n",
    "                print()\n",
    "    torch.save(Model.state_dict(), f\"InceptionNet_Model_Adversarial_Training-{epsilon}\")\n",
    "    return Train_Accuracy_History, Val_Accuracy_History, Train_Loss_History, Val_Loss_History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_Model_History(Train_Accuracy_History , Val_Accuracy_History, Train_Loss_History, Val_Loss_History):\n",
    "    # plot Accuracy\n",
    "    plt.plot(Train_Accuracy_History, marker='o')\n",
    "    plt.plot(Val_Accuracy_History, marker='o')\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    # plot Loss\n",
    "    plt.plot(Train_Loss_History, marker='8')\n",
    "    plt.plot(Val_Loss_History, marker='8')\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Propagate_Network(AT_flag, epsilon = .001):\n",
    "    Inception_Model = inception_model(out_c=10).to(device=Device)\n",
    "    Cross_Entropy_Criterion = nn.CrossEntropyLoss()\n",
    "    Adam_Optimizer = optim.Adam(Inception_Model.parameters(), lr=0.001)\n",
    "\n",
    "    if AT_flag == 1:\n",
    "        Train_Accuracy_History, Val_Accuracy_History, Train_Loss_History, Val_Loss_History = Train_Validate_Model_Adversarial_Training(Model = Inception_Model, Train_Loader = trainloader,Val_Loader = valloader,Num_Of_Train_Samples = len(trainset),\n",
    "                            Num_Of_Val_Samples = len(valset),Criterion = Cross_Entropy_Criterion,Optimizer = Adam_Optimizer,Num_Epochs = 100, epsilon=.001)\n",
    "    else:\n",
    "        Train_Accuracy_History, Val_Accuracy_History, Train_Loss_History, Val_Loss_History = Train_Validate_Model(Model = Inception_Model, Train_Loader = trainloader,Val_Loader = valloader,Num_Of_Train_Samples = len(trainset),\n",
    "                            Num_Of_Val_Samples = len(valset),Criterion = Cross_Entropy_Criterion,Optimizer = Adam_Optimizer,Num_Epochs = 100)\n",
    "    Plot_Model_History( Train_Accuracy_History, Val_Accuracy_History, Train_Loss_History, Val_Loss_History)\n",
    "Propagate_Network(1, .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:08<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy =  77.71000000000001 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def Model_Test(Test_Loader, model_name, Num_Of_Test_Samples):\n",
    "    Test_CIFAR = inception_model(out_c=10).to(device=Device)\n",
    "    Test_CIFAR.load_state_dict(torch.load(model_name))\n",
    "    Test_CIFAR.eval().to(Device)\n",
    "    Num_Of_Correct_Predicted = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_samples,targets in tqdm(Test_Loader):\n",
    "            batch_samples, targets = batch_samples.to(Device), targets.to(Device)\n",
    "            # Model Predictions\n",
    "            Predictions = Test_CIFAR(batch_samples)\n",
    "            \n",
    "            _, Predicted_Class = Predictions.max(1)\n",
    "            \n",
    "            # Calc number of samples that predicted correctly\n",
    "            Num_Of_Correct_Predicted += (Predicted_Class == targets).float().sum().item()\n",
    "    Test_Accuracy = Num_Of_Correct_Predicted / Num_Of_Test_Samples\n",
    "    print(\"Test Accuracy = \", Test_Accuracy*100 , \"%\")\n",
    "Model_Test(Test_Loader= testloader, model_name='./InceptionNet_Model', Num_Of_Test_Samples=10000)\n",
    "Model_Test(Test_Loader= testloader, model_name='./InceptionNet_Model_Adversarial_Training', Num_Of_Test_Samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_CIFAR = inception_model(out_c=10)#.to(device=Device);\n",
    "Test_CIFAR.load_state_dict(torch.load('./InceptionNet_Model'))\n",
    "Test_CIFAR.eval();#.to(Device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_attacks(attack, model, att_name):\n",
    "        images = ((torch.stack([transform(i) for i in testset.data]) + 1) / 2)\n",
    "        adv_images = (attack(images, torch.Tensor(testset.targets).long()))\n",
    "        targets_all = np.array(testset.targets)\n",
    "\n",
    "        Num_Of_Correct_Predicted = 0\n",
    "        with torch.no_grad():\n",
    "                for idx in zip(range(0,len(adv_images),128),range(128,len(adv_images)+128,128)):\n",
    "                        batch_samples, targets = (adv_images[idx[0]:idx[1]]*2) - 1, torch.Tensor(targets_all[idx[0]:idx[1]])#.to(Device)\n",
    "                #         # Model Predictions\n",
    "                        Predictions = model(batch_samples)\n",
    "                        \n",
    "                        _, Predicted_Class = Predictions.max(1)\n",
    "                        \n",
    "                        # Calc number of samples that predicted correctly\n",
    "                        Num_Of_Correct_Predicted += (Predicted_Class == targets).float().sum().item()\n",
    "        Test_Accuracy = Num_Of_Correct_Predicted / 10000\n",
    "        print(f\"Test Accuracy with {att_name} attack = \", Test_Accuracy*100 , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacks = [\n",
    "    torchattacks.VANILA(Test_CIFAR),\n",
    "    torchattacks.GN(Test_CIFAR),\n",
    "    torchattacks.FGSM(Test_CIFAR, eps=8/255),\n",
    "    torchattacks.PGD(Test_CIFAR, eps=8/255, alpha = 2/255, random_start=True),\n",
    "    torchattacks.CW(Test_CIFAR,  c=1, kappa=0),\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with VANILA attack =  77.72 %\n",
      "Test Accuracy with GN attack =  70.62 %\n",
      "Test Accuracy with FGSM attack =  44.91 %\n",
      "Test Accuracy with PGD attack =  44.11 %\n",
      "Test Accuracy with CW attack =  75.44 %\n"
     ]
    }
   ],
   "source": [
    "for a,n in zip(attacks,[\"VANILA\",\"GN\", \"FGSM\", \"PGD\", \"CW\"]):\n",
    "    eval_attacks(a, Test_CIFAR, n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_CIFAR = inception_model(out_c=10)#.to(device=Device);\n",
    "Test_CIFAR.load_state_dict(torch.load('./InceptionNet_Model_Adversarial_Training'))\n",
    "Test_CIFAR.eval();#.to(Device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with VANILA attack =  62.53999999999999 %\n",
      "Test Accuracy with GN attack =  55.94 %\n",
      "Test Accuracy with FGSM attack =  54.459999999999994 %\n",
      "Test Accuracy with PGD attack =  55.85 %\n",
      "Test Accuracy with CW attack =  61.94 %\n"
     ]
    }
   ],
   "source": [
    "for a,n in zip(attacks,[\"VANILA\",\"GN\", \"FGSM\", \"PGD\", \"CW\"]):\n",
    "    eval_attacks(a, Test_CIFAR, n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
